# Malware-Classification-and-Defence-Against-Adversarial-Attack
This is my final year project for engineering
## Scope:
After the computer boom of the 2000s, the number of people using personal machines grew exponentially. Unfortunately, so did the number of malware trying to hack into these machines to do various malicious tasks. Moreover, if we take a look at today’s online landscape, we can easily see a number of malware trying to infect our systems, held out by many security systems in place. These security systems not only use hashes to verify if a software is malware or not, but also are starting to use machine learning models to classify them. These machine learning models give us the probability of a software being malicious, and if it is above a certain threshold, then the user is notified of the same. Unfortunately, as machine learning started to weed out malicious code, hackers came up with a different technology. By using adversarial examples, hackers ensured that the model was trained in such a way that the number of false positives would be increased. This essentially confuses the model, and it allows harmful code to be executed by the system. This is the problem that I have solved in this project.

Firstly, I have compiled a dataset that comprises numerous features of 5000 data samples. These samples were then fed into a machine learning model using several existing algorithms and found out the best of them. Further, I identified the features that prominently impact the classification of the software. Using these features and the model, we created a hybrid classifier that was able to classify malware samples with greater accuracy as compared to existing algorithms. Lastly, I fed the model adversarial examples. This caused the accuracy to fall, as predicted. However, after training the model against such adversarial examples, the model became better at dealing with potential obfuscations in the features on which it was trained.

#### Dataset Description:
The dataset consisted of 5000 samples in total for training and testing with the bifurcation being around 3000 malware and 2000 benign samples. Each row represented a sample file. For every sample file 1002 column values existed with the first column being its MD5 Hash value and the last column being its label, i.e., whether it is malware or benign. The rest of the 1000 columns were the most common import functions invoked by all the sample files that were run in the cuckoo sandbox environment. Each of these columns had either 0 or 1 as their value. ‘0’ signified that the particular function wasn’t invoked by the given sample file. Whereas, ‘1’ indicated that the particular function was invoked by the given sample file.

#### Machine Learning Classifiers:
I began the modeling phase by identifying 3 different machine learning classifiers for our malware dataset. They are Random Forest, K - Nearest Neighbors and Naive Bayes. Each model was trained and subsequently tested on our malware dataset. Accuracy parameters for each of the classifiers were analyzed. Based on the results, I observed that Random Forest and KNN outperformed Naive Bayes significantly in terms of accuracy. To further bolster the accuracy in order to better classify malware samples I decided on a hybrid classifier which would leverage the benefits of both KNN and Random Forest. A deep neural network was used alongside KNN and Random Forest for build- ing the hybrid classifier. I made use of the predict_proba function where for each sample, the probability value of label 0 and 1 for KNN as well as Random Forest was obtained. So if for KNN, the predict_proba values were 0.92 and 0.08 for label 0 and 1 respectively, it would mean that KNN is 92% certain that the given sample is benign. With 2 values each for KNN and Random Forest we had 4 new features that could be used. These 4 output variables were added to the dataset of. These 1004 features were fed to a deep neural network with 7 layers in total. The last layer being the output layer. We used stochastic gradient descent as our optimizer and varied the learning rate, momentum and decay parameters of our neural network to arrive at the most optimum state of our hybrid classifier. Lastly I trained the neural network over 200 epochs and validated it with the test data.

#### Adversarial Examples and Adversarial Training
Adversarial Examples are generated by attackers in order to fool a machine learning classifier. They are small inputs provided to machine learning models that are intentionally designed by the attacker to induce an error on the part of the model. They are essentially features created out of small perturbations to the actual input features. In order to make the machine learning models robust against adversarial attacks I decided to generate certain adversarial examples and as a defence technique make use of Adversarial Training. But, To make changes in a dataset with a size of 1000 features would have been a huge task. Hence, I applied Principal Component Analysis (PCA) on the dataset to reduce the dimensionality of the dataset. By applying PCA, I made sure that all necessary data is retained, but at the same time the dimensionality is reduced. After reducing the features to 100, I calculated the feature importance score for all features. Based on the scores, I chose the 10 most important features out of 100 and created adversarial examples by changing the sign for the values of those features for 20% of the samples. As expected, accuracy for Random Forest and KNN drastically reduced when tested with this data. As a defence against adversarial attacks I decided to use Adversarial Training. In adversarial training, apart from the original data, the obfuscated data, i.e., the adversarial examples are also used for the training phase. This is done in order to make the machine learning model robust against perturbations in the data. Hence, the adversarial examples were then merged with the original data and adversarial training was performed for the defence part. Doing so, helped the Random Forest and KNN classifiers to reach close to their initial accuracies. Random Forest reached an accuracy of 89.73% and KNN reached an accuracy of 89.42%. Hence, the adversarial training proved to be an effective defence method as Random Forest and KNN regained their accuracy for the most part.

### Conclusion
Combining the attributes of K-Nearest Neighbors and Random Forest along with a deep neural network I wasd able to create a hybrid classifier that gave me an accuracy of 94.05%, which is close to 0.6% better than the next best classifier.
